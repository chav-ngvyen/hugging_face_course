{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCbMqieiChfg"
      },
      "source": [
        "# Handling multiple sequences (TensorFlow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgDoypIDChgn"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "4uqcf6p1ChhB",
        "outputId": "5aaa54fd-de38-4010-ee4e-80e9fb967763",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.5.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.7/dist-packages (0.2.2)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.7/dist-packages (4.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.1.97)\n",
            "Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "8PcmRzMyChhE",
        "outputId": "fb5bda5e-302c-4d12-aaef-8474d162a51f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_99']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-2.7276225,  2.8789392]], dtype=float32)>, hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = tf.constant(ids)\n",
        "# This line will fail.\n",
        "model(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "8969tmbQChjL",
        "outputId": "dec9e0ff-e478-4d90-9354-5cb04072822e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[  101  1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026\n",
            "   2878  2166  1012   102]], shape=(1, 16), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "leCfmR6fChjN",
        "outputId": "9120b1eb-a04d-47b3-dd58-a5e21ab6d5fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_119']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tf.Tensor(\n",
            "[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n",
            "   2166  1012]], shape=(1, 14), dtype=int32)\n",
            "Logits: tf.Tensor([[-2.7276225  2.8789392]], shape=(1, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "input_ids = tf.constant([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batched_ids = [ids, ids]\n",
        "print(batched_ids)"
      ],
      "metadata": {
        "id": "PbmoKT_UFEO_",
        "outputId": "d75192fd-faff-46dd-87ae-e3c7a07af421",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012], [1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "✏️ Try it out! Convert this batched_ids list into a tensor and pass it through your model. Check that you obtain the same logits as before (but twice)!"
      ],
      "metadata": {
        "id": "NE9DJ4BbFNNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batched_input_ids = tf.constant(batched_ids)\n",
        "print(\"Batched input IDs:\", batched_input_ids)"
      ],
      "metadata": {
        "id": "Z0o1h4s6Fk4j",
        "outputId": "e49134dc-9863-444a-e8ab-26c4f566c3fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batched input IDs: tf.Tensor(\n",
            "[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n",
            "   2166  1012]\n",
            " [ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n",
            "   2166  1012]], shape=(2, 14), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(batched_input_ids)\n",
        "print(\"Batched logits:\", output.logits)"
      ],
      "metadata": {
        "id": "c0zZTBPrHCPV",
        "outputId": "2f57cf97-107b-42d4-d8d4-800c8e38c0d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batched logits: tf.Tensor(\n",
            "[[-2.7276185  2.878935 ]\n",
            " [-2.7276206  2.878937 ]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yes"
      ],
      "metadata": {
        "id": "hYQXFrqKHLgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding the inputs"
      ],
      "metadata": {
        "id": "GbiavZjzHfUN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "zgeyo7SmChlb"
      },
      "outputs": [],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "CbPq939JChmR"
      },
      "outputs": [],
      "source": [
        "padding_id = 100\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, padding_id],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "l5kkbTlzChmY",
        "outputId": "9cc72262-df0e-483e-c339-45d948e7d0cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_139']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[ 1.5693678 -1.3894578]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 0.58030325 -0.41252738]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 1.569367  -1.3894578]\n",
            " [ 1.3373486 -1.2163193]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(model(tf.constant(sequence1_ids)).logits)\n",
        "print(model(tf.constant(sequence2_ids)).logits)\n",
        "print(model(tf.constant(batched_ids)).logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "NJ72ikYJChn4",
        "outputId": "6efb36e0-49ca-4b5a-fd77-c220f3dc56df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 1.569367   -1.3894578 ]\n",
            " [ 0.58029795 -0.4125215 ]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))\n",
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xkBHsJiVHogl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✏️ Try it out! Apply the tokenization manually on the two sentences used in section 2 (“I’ve been waiting for a HuggingFace course my whole life.” and “I hate this so much!”). Pass them through the model and check that you get the same logits as in section 2. Now batch them together using the padding token, then create the proper attention mask. Check that you obtain the same results when going through the model!"
      ],
      "metadata": {
        "id": "6-AUOTGSHpA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply tokenization separately"
      ],
      "metadata": {
        "id": "Gxw35KLJH5uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "p4xX1OySHxfV",
        "outputId": "d8c95481-e268-40af-b289-960ce3e6b091",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_159']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Not the same - this is what I got from session 2\n",
        "\n",
        "tf.Tensor(\n",
        "[[-1.5606971  1.6122824]\n",
        " [ 4.169232  -3.3464472]], shape=(2, 2), dtype=float32)"
      ],
      "metadata": {
        "id": "K9FNGob_NZ7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Logits 1:\", model(input_ids1).logits, \"Logits 2:\", model(input_ids2).logits)"
      ],
      "metadata": {
        "id": "_Iz8ymdzwVha",
        "outputId": "a46794e9-c820-47a0-9ea0-6126c9898785",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits 1: tf.Tensor([[-2.7276225  2.8789392]], shape=(1, 2), dtype=float32) Logits 2: tf.Tensor([[ 3.1248865 -2.6449811]], shape=(1, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Codes copied from section 2"
      ],
      "metadata": {
        "id": "dZX8BJyxxQss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "raw_tokens = tokenizer(raw_inputs)\n",
        "\n",
        "print(raw_tokens)\n",
        "# inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "yNkBJ4_1xU1d",
        "outputId": "5ef4eba5-aec1-46b2-ca3d-f37d54136a7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 1045, 5223, 2023, 2061, 2172, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This one has the 101 and 102 paddings\n",
        "\n"
      ],
      "metadata": {
        "id": "itcqDWgLD-M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence1 = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "sequence2 = \"I hate this so much.\"\n",
        "\n",
        "tokens1 = tokenizer.tokenize(sequence1)\n",
        "tokens2 = tokenizer.tokenize(sequence2)\n",
        "\n",
        "print(tokens1, \"\\n\", tokens2)"
      ],
      "metadata": {
        "id": "rZY68-2mCz7S",
        "outputId": "ac85b236-7611-476c-c741-0942c8fa7509",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.'] \n",
            " ['i', 'hate', 'this', 'so', 'much', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
        "ids2 = tokenizer.convert_tokens_to_ids(tokens2)\n",
        "\n",
        "input_ids1 = tf.constant(ids1)\n",
        "input_ids2 = tf.constant(ids2)\n",
        "\n",
        "print(\"Input IDs 1:\", input_ids1)\n",
        "print(\"Input IDs 2:\", input_ids2)"
      ],
      "metadata": {
        "id": "tCl0sSdhDv88",
        "outputId": "748ca44a-7ad4-40e1-c348-2a5e8f420c8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs 1: tf.Tensor(\n",
            "[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n",
            "  2166  1012], shape=(14,), dtype=int32)\n",
            "Input IDs 2: tf.Tensor([1045 5223 2023 2061 2172 1012], shape=(6,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This one lacks padding"
      ],
      "metadata": {
        "id": "tfXeK5dTD3N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens1_sb = tokenizer([sequence1])\n",
        "tokens2_sb = tokenizer([sequence2])\n",
        "\n",
        "print(tokens1_sb, \"\\n\", tokens2_sb)"
      ],
      "metadata": {
        "id": "R-QTUfD3EIwG",
        "outputId": "dba6a269-7ed1-4776-be6e-7260c6df3abf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]} \n",
            " {'input_ids': [[101, 1045, 5223, 2023, 2061, 2172, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# If I put sequence1 and sequence2 inside square brackets, I can preserve the 101 and 102 tokens"
      ],
      "metadata": {
        "id": "UDZR_m75ElyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logits for raw_tokens"
      ],
      "metadata": {
        "id": "WeHi70QBE5lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_tokens)"
      ],
      "metadata": {
        "id": "diPwXL26E7Yy",
        "outputId": "95a0d7a3-44de-4e93-98ad-473699bab58e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 1045, 5223, 2023, 2061, 2172, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_tokens['input_ids'])"
      ],
      "metadata": {
        "id": "xhCsv_7cGVbn",
        "outputId": "a994803e-f0a5-4e67-c2a1-d4dde9b78de4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 1045, 5223, 2023, 2061, 2172, 999, 102]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model(tf.constant(raw_tokens['input_ids'][0])).logits)\n",
        "print(model(tf.constant(raw_tokens['input_ids'][1])).logits)"
      ],
      "metadata": {
        "id": "gfLxmtaBHU2M",
        "outputId": "0300bb77-ac64-49c9-8a93-48f68d84ab25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[-1.5606974  1.612282 ]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 4.1692314 -3.3464477]], shape=(1, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#vs the ones without padding"
      ],
      "metadata": {
        "id": "UEUoUNFQJBal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model(input_ids1).logits)\n",
        "print(model(input_ids2).logits)\n"
      ],
      "metadata": {
        "id": "svcctI1BJEMH",
        "outputId": "479007bc-0972-48f2-d9f3-54f0effd2418",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[-2.7276225  2.8789392]], shape=(1, 2), dtype=float32)\n",
            "tf.Tensor([[ 3.1248865 -2.6449811]], shape=(1, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Both are not quite right. Need to add padding to the shorter sentence"
      ],
      "metadata": {
        "id": "Czk3Kv8UJPOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Pad the shorter sentence\n",
        "raw_tokens['input_ids'][1]= raw_tokens['input_ids'][1]+[0]*(len(raw_tokens['input_ids'][0])-len(raw_tokens['input_ids'][1]))\n",
        "\n",
        "print(raw_tokens['input_ids'])"
      ],
      "metadata": {
        "id": "EidjTqmLLTb4",
        "outputId": "6842cafb-5f02-4043-fd1e-f9cb2a6d15c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102], [101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now pad the shorter attention mask"
      ],
      "metadata": {
        "id": "Asthcg9BLfqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_tokens['attention_mask'])"
      ],
      "metadata": {
        "id": "YCTc7ln8JYcH",
        "outputId": "9f39bdac-4765-4e1c-aafd-fa3546a60605",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_tokens['attention_mask'][1])"
      ],
      "metadata": {
        "id": "8dosdXxvJddQ",
        "outputId": "ae69f46c-11b4-4c3a-df0e-a2bf8f26d37d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad the second attention mask\n",
        "raw_tokens['attention_mask'][1]= raw_tokens['attention_mask'][1]+[0]*(len(raw_tokens['attention_mask'][0])-len(raw_tokens['attention_mask'][1]))"
      ],
      "metadata": {
        "id": "XvXL1U7NJhCw"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(raw_tokens['attention_mask'][1])"
      ],
      "metadata": {
        "id": "NPwfDLk0J-lS",
        "outputId": "2d622626-5f9a-4ce8-ea24-e38cbaf91ddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.constant(raw_tokens['input_ids'][0]),\"\\n\")\n",
        "print(tf.constant(raw_tokens['attention_mask'][0]),\"\\n\")\n",
        "\n",
        "print(model(tf.constant(raw_tokens['input_ids'][0]), attention_mask = tf.constant(raw_tokens['attention_mask'][0])).logits)\n"
      ],
      "metadata": {
        "id": "aYYVEHM4KEN2",
        "outputId": "63f5207f-e171-46f0-8add-4e959c53be57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[  101  1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026\n",
            "  2878  2166  1012   102], shape=(16,), dtype=int32) \n",
            "\n",
            "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1], shape=(16,), dtype=int32) \n",
            "\n",
            "tf.Tensor([[-1.5606974  1.612282 ]], shape=(1, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.constant(raw_tokens['input_ids'][1]),\"\\n\")\n",
        "print(tf.constant(raw_tokens['attention_mask'][1]),\"\\n\")\n",
        "\n",
        "print(model(tf.constant(raw_tokens['input_ids'][1]), attention_mask = tf.constant(raw_tokens['attention_mask'][1])).logits)\n"
      ],
      "metadata": {
        "id": "nlIEg5LaK97d",
        "outputId": "2887d73d-5dc8-4a32-af03-5be8d66c0079",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[ 101 1045 5223 2023 2061 2172  999  102    0    0    0    0    0    0\n",
            "    0    0], shape=(16,), dtype=int32) \n",
            "\n",
            "tf.Tensor([1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0], shape=(16,), dtype=int32) \n",
            "\n",
            "tf.Tensor([[ 4.1692314 -3.3464475]], shape=(1, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yes, these are the same logits from session 2. That's because I included the 101 and 102 tokens from the model "
      ],
      "metadata": {
        "id": "JECfZjBtLx67"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZXAG2T_mL2T2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Handling multiple sequences (TensorFlow)",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}